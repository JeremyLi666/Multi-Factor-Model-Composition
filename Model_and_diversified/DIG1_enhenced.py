# -*- coding: utf-8 -*-
"""
ä¸»æŒ–æ˜ç¨‹åºï¼ˆæ¨¡æ¿åŒ–ç‰ˆï¼‰
ä¾èµ–ï¼š
- machine_lib.py ä¸­å·²åŠ å…¥ MachinelibTemplates ç±»
- config.py, fields.py ä¸åŸå·¥ç¨‹ä¿æŒä¸€è‡´
"""

import os
import random
import time
import asyncio
from datetime import datetime
import argparse

from machine_lib_v2 import *                    # ä½ åŸæœ‰çš„APIï¼šlogin/get_datafields/process_datafields/...
from machine_lib_v2 import MachinelibTemplates  # æ¨¡æ¿ç”Ÿæˆå™¨ç±»
from config import *                         # ä½ çš„å¸¸è§„é…ç½®
from fields import *                         # å­—æ®µæ˜ å°„ç­‰

from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn
from rich.console import Console

console = Console()


# ---------------------- åŸºäºæ¨¡æ¿çš„å€™é€‰ç”Ÿæˆ ----------------------
def _pick_template_alphas(model_type: str, group) -> list:
    """
    ä»å­—æ®µé›†åˆ `group` ç”Ÿæˆè¡¨è¾¾å¼ï¼›æ”¾å¤§äº§é‡çš„ç‰ˆæœ¬ã€‚
    æ”¯æŒ:
      - momentum_diverse      æ‰©å¤§çª—å£/åˆ†ç»„
      - twin / risk_compare   åŒä¸Š
      - combo_core            åˆå¹¶ä¸‰ç±»æ¨¡æ¿ï¼Œäº§é‡ä¸­é«˜
      - combo_heavy           å­—æ®µ/çª—å£/åˆ†ç»„å…¨æ”¾å¤§ï¼ˆè­¦æƒ•è¿‡å¤šï¼‰
    """
    # 1) æ‹‰å¹³ + å»é‡
    all_fields = list(dict.fromkeys(
        process_datafields(group, "matrix") + process_datafields(group, "vector")
    ))

    def filt(keys):
        kl = [k.lower() for k in keys]
        return [f for f in all_fields if any(k in f.lower() for k in kl)]

    # å­—æ®µæ—ï¼ˆæ”¾å®½ç­›é€‰ï¼Œä¸åª returns/closeï¼‰
    core_fields  = filt(["returns", "ret", "close", "open", "high", "low", "volume", "turnover", "cap"])
    if len(core_fields) < 50:
        # å­—æ®µä¸å¤Ÿå°±æŠŠå‰ 200 ä¸ªéƒ½æ‹‰ä¸Šï¼Œå¢åŠ è¦†ç›–é¢
        core_fields = (core_fields + all_fields)[:800]
    twin_fields  = filt(["volume", "turnover", "cap", "liquidity", "dollar"])
    risk_fields  = filt(["beta", "vol", "risk", "downside", "skew", "kurt"]) or core_fields[:50]
    news_fields  = filt(["nws", "news", "snt", "sentiment"])
    analyst_fields = filt(["anl", "analyst", "estimate", "recom"]) or core_fields[:50]
    pv_fields    = [x for x in all_fields if "close*volume" in x.lower()] or ["close*volume"]

    # 2) åˆ†ç»„ä¸çª—å£ï¼ˆæ”¾å¤§ï¼‰
    GROUPS_LITE   = ["sector", "industry"]
    GROUPS_FULL   = MachinelibTemplates.get_builtin_groups()  # å¸‚åœº/å›½å®¶/è¡Œä¸šå±‚çº§ + å„ç±»åˆ†æ¡¶
    SHORT_BIG     = [5, 10, 20, 22, 33]                       # ä¹‹å‰åªæœ‰ 5,22
    LONG_BIG      = [66, 90, 120, 180, 252, 504]              # ä¹‹å‰åªæœ‰ 66,120
    LONG_MEDIUM   = [66, 120, 252]

    key = (model_type or "").strip().lower().replace(" ", "_")
    out = []

    if key in ("momentum_diverse", "momentum", "mom_div", ""):
        out = MachinelibTemplates.build_momentum_diverse(
            fields=core_fields,
            groups=GROUPS_FULL,                 # æ‰©åˆ°å…¨ç»„
            short_windows=SHORT_BIG,            # æ‰©åˆ° 5 ä¸ªçŸ­çª—
            long_windows=LONG_BIG,              # æ‰©åˆ° 6 ä¸ªé•¿çª—
            wrap=(120, 4.0, True)
        )

    elif key in ("twin", "pair", "corr"):
        out = MachinelibTemplates.build_twin_ops(
            primary_fields=core_fields[:80],    # æ§åˆ¶å¯¹æ•°ï¼Œé¿å…ç»„åˆçˆ†ç‚¸
            twin_fields=(twin_fields or core_fields)[:80],
            windows=LONG_MEDIUM,
            twin_ops=["ts_corr", "ts_covariance"],
            groups=GROUPS_FULL,
            wrap=(120, 4.0, True)
        )

    elif key in ("risk_compare", "risk_group"):
        out = MachinelibTemplates.build_risk_group_compare(
            risk_fields=risk_fields[:120],
            groups=GROUPS_FULL,
            compare_ops=MachinelibTemplates.GROUP_COMPARE_OPS,
            wrap=(120, 4.0, True)
        )

    elif key in ("combo_core",):
        # åˆå¹¶ä¸‰ç±»æ¨¡æ¿ï¼ˆæ•°é‡é€šå¸¸å‡ åƒåˆ°ä¸€ä¸¤ä¸‡ï¼Œè§†å­—æ®µè€Œå®šï¼‰
        a1 = MachinelibTemplates.build_momentum_diverse(
            fields=core_fields,
            groups=GROUPS_FULL,
            short_windows=SHORT_BIG,
            long_windows=LONG_MEDIUM,
            wrap=(120, 4.0, True)
        )
        a2 = MachinelibTemplates.build_twin_ops(
            primary_fields=core_fields[:60],
            twin_fields=(twin_fields or core_fields)[:60],
            windows=LONG_MEDIUM,
            twin_ops=["ts_corr", "ts_covariance"],
            groups=GROUPS_FULL,
            wrap=(120, 4.0, True)
        )
        a3 = MachinelibTemplates.build_risk_group_compare(
            risk_fields=risk_fields[:100],
            groups=GROUPS_FULL,
            compare_ops=["group_neutralize", "group_rank", "group_zscore"],
            wrap=(120, 4.0, True)
        )
        out = list(dict.fromkeys(a1 + a2 + a3))  # å»é‡ä¿æŒé¡ºåº

    elif key in ("combo_heavy",):
        # é‡å£å‘³ï¼šå­—æ®µ/åˆ†ç»„/çª—å£å…¨æ‹‰æ»¡ï¼ˆå°å¿ƒ OOMï¼‰
        a1 = MachinelibTemplates.build_momentum_diverse(
            fields=core_fields[:300],
            groups=GROUPS_FULL,
            short_windows=SHORT_BIG,
            long_windows=LONG_BIG,
            wrap=(120, 4.0, True)
        )
        a2 = MachinelibTemplates.build_twin_ops(
            primary_fields=core_fields[:150],
            twin_fields=(twin_fields or core_fields)[:150],
            windows=LONG_BIG,
            twin_ops=["ts_corr", "ts_covariance"],
            groups=GROUPS_FULL,
            wrap=(120, 4.0, True)
        )
        a3 = MachinelibTemplates.build_risk_group_compare(
            risk_fields=risk_fields[:200],
            groups=GROUPS_FULL,
            compare_ops=MachinelibTemplates.GROUP_COMPARE_OPS,
            wrap=(120, 4.0, True)
        )
        out = list(dict.fromkeys(a1 + a2 + a3))

    elif key in ("news_corr", "news_volume"):
        if not news_fields:
            print(datetime.now(), "æœªå‘ç°æ–°é—»å­—æ®µï¼Œé€€å› momentum_diverse")
            return _pick_template_alphas("momentum_diverse", group)
        out = MachinelibTemplates.build_news_return_corr(
            news_fields=news_fields[:60],
            windows=[120, 180, 252],
            groups=GROUPS_FULL,
            wrap=(120, 4.0, True)
        )

    elif key in ("analyst_reg", "analyst_regression"):
        out = MachinelibTemplates.build_analyst_regression(
            analyst_fields=analyst_fields[:80],
            pv_fields=list(dict.fromkeys(pv_fields + ["close*volume"]))[:10],
            w1=22, w2=120,
            groups=["country", "industry", "sector"],
            wrap=(120, 4.0, True)
        )

    elif key in ("fcf", "fundamental_fcf"):
        out = MachinelibTemplates.build_fcf_ratio(
            fcf_field="fcf", mkt_cap_field="market_cap",
            smooth_window=60, groups=["sector","industry"], wrap=(120, 4.0, True)
        )

    elif key in ("vector_neut", "risk_neutral"):
        risk_ref = next((x for x in all_fields if "risk" in x.lower()), "risk70")
        out = MachinelibTemplates.build_vector_neutralized(
            fields=core_fields[:150],
            risk_field=risk_ref,
            windows=[5,10,20,22,33],
            groups_after=["sector", "bucket(rank(cap), range='0.1, 1, 0.1')"],
            wrap=(120, 4.0, True)
        )

    elif key in ("explore", "smoke"):
        out = MachinelibTemplates.build_explore_simple(core_fields[:200], wrap=(120, 4.0, True))

    else:
        # æœªè¯†åˆ«ï¼šé»˜è®¤ç”¨æ”¾å¤§çš„åŠ¨é‡åˆ†æ­§
        out = MachinelibTemplates.build_momentum_diverse(
            fields=core_fields,
            groups=GROUPS_FULL,
            short_windows=SHORT_BIG,
            long_windows=LONG_BIG,
            wrap=(120, 4.0, True)
        )

    # æœ€åä¸€é“å»é‡
    return list(dict.fromkeys(out))


# ---------------------- ä¸»ä»»åŠ¡ ----------------------

def run_task(dataset_id, region, delay, instrumentType, universe, n_jobs, tag=None, model_type="momentum_diverse"):
    delay = int(delay)
    n_jobs = int(n_jobs)

    print(datetime.now(), "================= å›æµ‹ä»»åŠ¡å¯åŠ¨ =================")
    print(datetime.now(), f"dataset_id:       {dataset_id}")
    print(datetime.now(), f"region:           {region}")
    print(datetime.now(), f"delay:            {delay}")
    print(datetime.now(), f"instrumentType:   {instrumentType}")
    print(datetime.now(), f"universe:         {universe}")
    print(datetime.now(), f"n_jobs:           {n_jobs}")
    print(datetime.now(), f"tag:              {tag}")
    print(datetime.now(), f"model_type:       {model_type}")
    print("================================================")

    print(datetime.now(), "ç™»å½•ä¸­...")
    s = login()
    print(datetime.now(), "ç™»å½•æˆåŠŸï¼Œæ‹‰å–å­—æ®µä¸­...")

    group = get_datafields(s=s, dataset_id=dataset_id, region=region, delay=delay, universe=universe)
    s.close()

    if group is None or len(group) == 0:
        print(datetime.now(), "å­—æ®µä¸ºç©ºï¼Œè·³è¿‡ä»»åŠ¡")
        return

    if tag is None:
        tag = f"{region}_{delay}_{instrumentType}_{universe}_{dataset_id}_{model_type}"

    completed_file_path = os.path.join(RECORDS_PATH, f"{tag}_simulated_alpha_expression.txt")
    completed_alphas = read_completed_alphas(completed_file_path)

    # åŸºäºæ¨¡æ¿ç”Ÿæˆå€™é€‰
    print(datetime.now(), f"ğŸ§± ä½¿ç”¨æ¨¡æ¿ [{model_type}] ç”Ÿæˆå€™é€‰è¡¨è¾¾å¼...")
    raw_alpha_list = _pick_template_alphas(model_type=model_type, group=group)
    print(datetime.now(), f"âœ… æ¨¡æ¿ç”Ÿæˆå®Œæˆï¼Œå…± {len(raw_alpha_list)} æ¡")

    # è¿‡æ»¤å·²å®Œæˆ
    alpha_list = [alpha for alpha in raw_alpha_list if alpha not in completed_alphas]

    if len(alpha_list) == 0:
        print(datetime.now(), f"{tag} æ‰€æœ‰è¡¨è¾¾å¼å·²å®Œæˆï¼Œè·³è¿‡")
        return

    # æ‰“ä¹±+æˆªæ–­
    random.shuffle(alpha_list)
    alpha_list = alpha_list[:1000]  # é˜²å¡æ­»ä¿é™©

    print(datetime.now(), f"ğŸ¯ å¾…å›æµ‹è¡¨è¾¾å¼æ•°ï¼š{len(alpha_list)} / {len(raw_alpha_list)}")

    # ç»„è£…æäº¤å‚æ•°
    region_list = [(region, universe)] * len(alpha_list)
    decay_list = [random.randint(0, 10) for _ in alpha_list]
    delay_list = [delay] * len(alpha_list)
    neut = 'SUBINDUSTRY'

    batch_size = 80
    print(datetime.now(), f"â© å…±éœ€å›æµ‹ï¼š{len(alpha_list)} ä¸ªï¼Œæ‰¹å¤§å°ï¼š{batch_size}")

    with Progress(
        TextColumn("[bold green]å›æµ‹è¿›åº¦"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        transient=True
    ) as progress:
        task = progress.add_task("æäº¤ä¸­...", total=len(alpha_list))

        for i in range(0, len(alpha_list), batch_size):
            batch = alpha_list[i:i + batch_size]
            region_batch = region_list[i:i + batch_size]
            decay_batch = decay_list[i:i + batch_size]
            delay_batch = delay_list[i:i + batch_size]

            console.print(f"{datetime.now()} æäº¤ç¬¬ {i // batch_size + 1} æ‰¹ï¼ˆ{len(batch)} æ¡ï¼‰...")

            asyncio.run(simulate_multiple_tasks(
                batch, region_batch, decay_batch, delay_batch,
                tag, neut, [], n=n_jobs
            ))

            progress.update(task, advance=len(batch))
            time.sleep(15)

    print(datetime.now(), "âœ… æœ¬è½®ä»»åŠ¡å®Œæˆï¼Œç­‰å¾…10ç§’...")
    time.sleep(10)
    print(datetime.now(), "å¼€å§‹ä¸‹ä¸€è½®")


# ---------------------- å¯åŠ¨å…¥å£ ----------------------
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="æ¨¡æ¿åŒ–æŒ–æ˜å™¨")
    parser.add_argument("--dataset_id", default="news18")
    parser.add_argument("--region", default="EUR")
    parser.add_argument("--delay", type=int, default=1)
    parser.add_argument("--instrumentType", default="EQUITY")
    parser.add_argument("--universe", default="TOP2500")
    parser.add_argument("--n_jobs", type=int, default=6)
    parser.add_argument("--tag", default="news18model")
    parser.add_argument("--model_type", default="news_corr",
                        help="option1 | momentum_diverse | twin | vol_div | risk_compare | vector_neut | mean_dev | news_corr | fcf | analyst_reg | explore")
    parser.add_argument("--loop", action="store_true", help="æ˜¯å¦å¾ªç¯æ‰§è¡Œ")

#æ¨¡æ¿ç±»å‹è§£é‡Šï¼š

#å¤šä¸ªæ•°æ®å­—æ®µçš„

# option1ï¼ˆæœŸæƒIVå·® + é—¨æ§ï¼‰
# æ¨¡æ¿ï¼štrade_when(pcr_oi_d>1, group_neutralize(ts_delta(IV_call - IV_put, L), densify(G)), -1)
# å«ä¹‰ï¼šçœ‹å¤š/çœ‹ç©ºååº¦çš„çŸ­æœŸå˜åŒ–ï¼Œåœ¨è¡Œä¸šæˆ–ç»†åˆ†ç»„å†…åšä¸­æ€§åŒ–ï¼ŒPCR>1æ—¶æ‰ç”Ÿæ•ˆï¼Œé™å™ªã€‚
# å­—æ®µå»ºè®®ï¼šiv_call_*ã€iv_put_*ï¼ˆæ—¥å˜åŠ¨/æ°´å¹³ï¼‰ï¼Œå¯é€‰ pcr_oi/pcr_volï¼›ç¼ºå°‘å³åˆ«ç”¨æ­¤æ¨¡å¼ã€‚

# momentum_diverseï¼ˆåŠ¨é‡åˆ†æ­§ï¼‰
# æ¨¡æ¿ï¼š[group_zscore(ts_zscore(X, w1), G) - group_zscore(ts_zscore(X, w2), G)] ä¸” w2>w1
# å«ä¹‰ï¼šçŸ­æœŸä¸ä¸­é•¿æœŸä¿¡å·çš„å·®åˆ†ï¼Œæ•æ‰åè½¬æˆ–åŠ é€Ÿæ®µã€‚
# å­—æ®µå»ºè®®ï¼šreturnsã€closeï¼Œä¹Ÿå¯ç”¨ volume/closeã€turnover æ´¾ç”Ÿåºåˆ—ã€‚

# twinï¼ˆæˆå¯¹ç›¸å…³/åæ–¹å·®ï¼‰
# æ¨¡æ¿ï¼šgroup_neutralize(ts_corr(X, Y, L) | ts_covariance(X, Y, L), densify(G))
# å«ä¹‰ï¼šä¸¤å˜é‡åœ¨æ—¶é—´çª—å†…çš„è”åŠ¨å¼ºåº¦ï¼Œç»„å†…å»é™¤ç»“æ„æ€§å·®å¼‚ã€‚
# å­—æ®µå»ºè®®ï¼š(returns, volume)ã€(returns, cap)ã€(close, volume) ç­‰å¸¸è§é‡ä»·/è§„æ¨¡å¯¹ã€‚

# vol_divï¼ˆæ³¢åŠ¨ç‡åˆ†æ­§ï¼‰
# æ¨¡æ¿ï¼šgroup_neutralize(power(ts_mean(abs(X), L),2) - power(ts_mean(X, L),2), densify(G))
# å«ä¹‰ï¼šå¹…åº¦ vs æ–¹å‘çš„èƒ½é‡å·®ï¼Œåå‘æ•æ‰éœ‡è¡ä¸å™ªå£°ä¸»å¯¼æ®µã€‚
# å­—æ®µå»ºè®®ï¼šreturns ä¸ºä¸»ï¼›å¯è¯• spread(ask,bid)ã€hi_lo_range ç­‰æ³¢åŠ¨ä»£ç†ã€‚

# vector_neutï¼ˆå‘é‡ä¸­æ€§åŒ–ï¼‰
# æ¨¡æ¿ï¼šwrap(vector_neut(ts_zscore(X, L), ts_backfill(RISK_VEC, 120)))ï¼Œå¯å†å¯¹ G å¤šçº§ä¸­æ€§åŒ–
# å«ä¹‰ï¼šä»ç›®æ ‡ä¿¡å·ä¸­å‰¥ç¦»ç»™å®šé£é™©å‘é‡çš„æš´éœ²ï¼Œä¿ç•™â€œçº¯é˜¿å°”æ³•â€ã€‚
# å­—æ®µå»ºè®®ï¼šX=returns/closeï¼›RISK_VEC ç”¨ç»¼åˆé£é™©å¦‚ risk70ã€æˆ–æ‹¼æ¥çš„é£æ ¼è´å¡”é›†åˆã€‚

# fcfï¼ˆè‡ªç”±ç°é‡‘æµç‡ï¼‰
# æ¨¡æ¿ï¼šgroup_neutralize(-ts_mean(winsorize(ts_backfill(FCF/MC,120), std=4), 60), densify(G))
# å«ä¹‰ï¼šä»·å€¼/ç°é‡‘å›æŠ¥åå¥½ï¼Œåå‘å–å€¼ä»¥é€‚é…åšå¤šâ€œä¾¿å®œâ€èµ„äº§ã€‚
# å­—æ®µå»ºè®®ï¼šfcfã€market_cap æˆ–ç­‰ä»·å£å¾„çš„ FCF ä¸å¸‚å€¼å­—æ®µï¼›æ³¨æ„å£å¾„ä¸€è‡´æ€§ä¸æ»åã€‚

# analyst_regï¼ˆåˆ†æå¸ˆå›å½’ï¼‰
# æ¨¡æ¿ï¼š-ts_mean(group_neutralize(ts_regression(ts_zscore(ANL,w1), ts_zscore(PV,w1), w2), densify(G)), w2)
# å«ä¹‰ï¼šåˆ†æå¸ˆå˜é‡å¯¹ä»·æ ¼/æˆäº¤å¼ºåº¦çš„è§£é‡Šæ®‹å·®ï¼Œç»„å†…æ ‡å‡†åŒ–åå¹³æ»‘ã€‚
# å­—æ®µå»ºè®®ï¼šANL=anl* å®¶æ—ï¼ˆä¸Šè°ƒ/ä¸‹è°ƒã€è¦†ç›–ã€ä¸€è‡´é¢„æœŸå˜åŒ–ç­‰ï¼‰ï¼›PV=close*volume æˆ– dollar_volumeã€‚




#å•ä¸ªæ•°æ®å­—æ®µå°±å¯ä»¥çš„

# risk_compareï¼ˆé£é™©åˆ†ç»„æ¯”è¾ƒï¼‰
# æ¨¡æ¿ï¼šgroup_{rank|zscore|neutralize|scale}(RISK, densify(G))
# å«ä¹‰ï¼šç›´æ¥åœ¨åˆ†ç»„ç»´åº¦ä¸Šæ¯”è¾ƒé£é™©æš´éœ²å¹¶æ ‡å‡†åŒ–ã€‚
# å­—æ®µå»ºè®®ï¼šbeta_60/120ã€vol_20/60ã€idioriskã€downside_volã€skew/kurt ç­‰é£é™©å› å­ã€‚

# mean_devï¼ˆå‡å€¼åç¦»ï¼‰
# æ¨¡æ¿ï¼šgroup_neutralize((X - ts_mean(X, L))/max(abs(ts_mean(X, L)), eps), densify(G))
# å«ä¹‰ï¼šç›¸å¯¹è‡ªèº«å‡å€¼çš„åç¦»åº¦ï¼Œé€‚åˆå‡å€¼å›å½’/è¿‡åº¦å»¶ä¼¸è¯†åˆ«ã€‚
# å­—æ®µå»ºè®®ï¼šreturnsã€close/maã€factor_raw ç­‰å•å˜é‡æ—¶åºã€‚

# news_corrï¼ˆæ–°é—»Ã—æ”¶ç›Šç›¸å…³ï¼‰
# æ¨¡æ¿ï¼šgroup_neutralize(ts_corr(NEWS, returns, L), densify(G))
# å«ä¹‰ï¼šæƒ…ç»ªä¸ä»·æ ¼çš„è€¦åˆå¼ºåº¦ï¼Œåå‘ä¸­æœŸã€‚
# å­—æ®µå»ºè®®ï¼šnws*ã€snt*ã€news18/77 è¿™ç±»æ–°é—»/æƒ…ç»ªèšåˆå­—æ®µï¼›æ²¡æœ‰å°±åˆ«é€‰è¿™æ¨¡å¼ã€‚

# exploreï¼ˆæ“ä½ å¦ˆæ¨¡æ¿ï¼‰
# æ¨¡æ¿ï¼šwrap(zscore(ts_delta(rank(ts_zscore(X,60)), 5)))
# å«ä¹‰ï¼šå¿«é€Ÿæ“ä½ å¦ˆçš„äºŒå±‚åŠ¨é‡æ¡†æ¶ï¼Œç”¨äºç­›åº•å™ªå­—æ®µçš„å¯ç”¨æ€§ã€‚
# å­—æ®µå»ºè®®ï¼šreturns èµ·æ­¥ï¼›ä¹Ÿå¯ç”¨ä»»æ„â€œå¯ç–‘ä½†æƒ³è¯•â€çš„åŸå§‹åºåˆ—ã€‚


    args = parser.parse_args()

    if args.loop:
        while True:
            run_task(
                dataset_id=args.dataset_id,
                region=args.region,
                delay=args.delay,
                instrumentType=args.instrumentType,
                universe=args.universe,
                n_jobs=args.n_jobs,
                tag=args.tag,
                model_type=args.model_type
            )
    else:
        run_task(
            dataset_id=args.dataset_id,
            region=args.region,
            delay=args.delay,
            instrumentType=args.instrumentType,
            universe=args.universe,
            n_jobs=args.n_jobs,
            tag=args.tag,
            model_type=args.model_type
        )
